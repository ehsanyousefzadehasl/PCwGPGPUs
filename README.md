# Parallel Computing with GPGPUs
In this repository, I summarized what I reviewed and learned from parallel comptuing with the help of GPGPUs. If you are starting to learn about parallel computing and GPGPUs, "Intro to Parallel Computing", which nowadays is a little out-of-date, but still helpful, by Prof. John Ownes from UC Davis would be a real eye-opener.

## Introduction
In this very first section, you will get familiar with a little story 
### Narrative History!
In this very short and concise introduction, I just want to summarize the reasons why we moved toward parallelism, and why GPGPUs got famous. The story starts 1948 (I can hardly remember, I do not think you would be able to remember anything) when Von Neumann architecture became the mainstream. Following this architecture, computer architectures started desigining and building processors that were fetching data from memories, processing, then writing back to memories. They encountered several challenges, but tried to find solutions for them. One of the challenges was the processor-memory performance gap. Computer architects for addressing this challenge focused on architectural techniques like caching, pre-fetching, multi-threding, PIM. The other one was memory wall when computer architects were not able to improve the performance simply by increasing the working frequency of chips due to the dennard scaling breakdown. So, they steered the computer architecture trend toward Parallelism. This time instead of complex large processor cores, they were desigining more simple processors working together. This architecture increased performance, and power efficiency by providing more operations per watt. Indeed, they focused on throughput (on large cores their focus was on latency) in these architectures. The only of parallel systems was the programmability hardship which was posed on programmers. Most of the time, it is challenging for a programmer who is used to develop serial program, to switch to a new thinking paradigm and develop parallel programs!

## Compute Unified Device Architecture (CUDA)
It is a parallel computing platform and API created by NVIDIA allowing developers to use a CUDA-enabled GPU for general purpose processing. This approach is termed as GPGPU (General Purpose GPU). The CUDA platform is a software layer providing a direct access to the GPU's virtual instruction set (PTX) and parallel computational elements, for the execution of kernels which also are called computer kernels. This platform is designed to work with programming languages like C, C++, Fortran. As a result, programming with CUDA is much easier than prior APIs, like Direct3D and OpenGL, to it.

This repository's goal is working with this platform. For working with this platform on Windows, you must install Microsoft Visual Studio Code alongside Nvidia's CUDAtoolkit. Nvidia Nsight is an application development environment which brings GPU computing into Microsoft Visual Studioallows you to build and debug integrated GPU kernels and native CPU code as well as inspect the state of the CPU, GPU, and memory. You do it because you need a c/c++ compiler beside NVCC. But, on a Linux OS due to the built-in compilers, you don't need to Microsoft VS.

When we develop a cuda program (extension .cu), it is consisted of two parts: (1) one part runs on the processor which is usually called host processor, (2) the other one runs on a GPU, which is usually called device. The following figure shows how a cuda program runs on a system consisted of a CPU and a GPU.
